{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import re\n",
    "df_ordered = pd.read_csv(\"../code/ordered.csv\")\n",
    "df_severeWeather = pd.read_csv(\"../code/severeWeather.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project I Report: Inferential Statistics\n",
    "\n",
    "From the previous report we observe the following:\n",
    "\n",
    "In general:\n",
    "\n",
    "* In extreme weather days, the sales record for item 45 dropped significantly. \n",
    "* In extreme weather days, the sales record for item 9 dropped slightly. \n",
    "* In extreme weather days, the sales record for item 44 dropped slightly.\n",
    "* In extreme weather days, people buy item 5 more and they often do so before extreme weather comes.\n",
    "* In extreme weather days, people buy more staff apart from item 44, item 45 and item 9, for last 6 items in top 10 increased by roughly 2 item per day\n",
    "* People buy item 93 more on the day of extreme weather. They also buy this item before an extreme weather.\n",
    "* when facing a long time extreme weather event, people do less shopping on item 5, 45, 44\n",
    "* Even when it is a sunny day, the sales record close to bad weather still differ from normal case, with item 5 being the best seller and item 45 at the third place.\n",
    "\n",
    "Specifically for item 5:\n",
    "\n",
    "* Year: Sales record steady goes down given the year.\n",
    "* Month: The month record is more even and diverse. It is hard to find a clear pattern.\n",
    "* Weekday: People tend to buy more on weekends. On weekdays, Monday and Friday see more selling than others.\n",
    "* Rainfall/Snowfall: People tend to buy item 5 on a sunny day. But when facing major weather events people will go and buy them as well. \n",
    "* Temperature: It can be observed that during normal days people tend to buy less item 5 when the temperature is between -16 to 38 $^{\\circ}$F, and between 64 $^{\\circ}$F to 76 $^{\\circ}$F. When there is a major weather event, however, the confidence interval becomes large enough to affect this conclusion from a statistical point of view\n",
    "\n",
    "These observations are going to be tested in this report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I get tools ready for the task: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define some useful functions first\n",
    "\n",
    "def perm_diff(data1,data2,targetfun):\n",
    "    conc_data = np.concatenate((data1,data2))\n",
    "    value = targetfun(data1) - targetfun(data2)\n",
    "    value_diff = np.empty(10000)\n",
    "\n",
    "    for i in range(10000):\n",
    "        perm_data = np.random.permutation(conc_data)\n",
    "        perm_data1 = perm_data[:len(data1)]\n",
    "        perm_data2 = perm_data[len(data1):]\n",
    "        \n",
    "        perm_value1 = targetfun(perm_data1)\n",
    "        perm_value2 = targetfun(perm_data2)\n",
    "        value_diff[i] = perm_value1 - perm_value2\n",
    "        \n",
    "        if value>0:\n",
    "            p = np.sum(value_diff > value)/float(len(value_diff))\n",
    "        else:\n",
    "            p = np.sum(value_diff < value)/float(len(value_diff))\n",
    "    print \"p value:\",  p\n",
    "    print \"value\", value\n",
    "    print \"99% null hypothesis interval:\",  np.percentile(value_diff, [0.5, 99.5])\n",
    "    \n",
    "def bsfromfunc(observes,targetfunc):\n",
    "    value = targetfunc(observes)\n",
    "    bs_target = np.empty(10000)\n",
    "    for i in range(10000):\n",
    "        bs_sample = np.random.choice(observes,size=len(observes))\n",
    "        bs_target[i] = targetfunc(bs_sample)\n",
    "        \n",
    "    print 'value, ', value\n",
    "    print '99% interval, ', np.percentile(bs_target, [0.5, 99.5])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In extreme weather days, the sales record for item 45 dropped significantly.\n",
    "\n",
    "I perform a null hypotheses test for this question. The hypothesis is: there is no difference for sales record of item 45 between extreme weather days and normal days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p value: 0.0\n",
      "value -8.5177407805\n",
      "99% null hypothesis interval: [-1.80311776  1.90913812]\n"
     ]
    }
   ],
   "source": [
    "def item_sales_inferential(item):\n",
    "\n",
    "    mask_event = ((df_ordered['WEvent'] == 1) & (df_ordered['item_nbr'] == item))\n",
    "    mask_no_event = ((df_ordered['WEvent'] == 0) & (df_ordered['item_nbr'] == item))\n",
    "\n",
    "    data1 = list(df_ordered.loc[mask_event, 'units'])\n",
    "    data2 = list(df_ordered.loc[mask_no_event, 'units'])\n",
    "\n",
    "    perm_diff(data1,data2,np.mean)\n",
    "\n",
    "item_sales_inferential(45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The p-value is 0, meaning the null hypothesis does not stand. Therefore, in extreme weather days, the sales record for item 45 did drop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In extreme weather days, the sales record for item 9 dropped slightly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I perform a null hypotheses test for this question. The hypothesis is: there is no difference for sales record of item 9 between extreme weather days and normal days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p value: 0.0181\n",
      "value -1.78536037982\n",
      "99% null hypothesis interval: [-2.2010421   2.26200586]\n"
     ]
    }
   ],
   "source": [
    "item_sales_inferential(9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is actually close to 2% chance for null hypotheses to stand. Under 5% acceptance, we can still reject this null hypothesis. Therefore, we can still say the sales record for item 9 dropped during extreme weather days, although slightly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In extreme weather days, the sales record for item 44 dropped slightly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I perform a null hypothesis test for this question. The hypothesis is: there is no difference for sales record of item 44 between extreme weather days and normal days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p value: 0.24\n",
      "value -0.66953605725\n",
      "99% null hypothesis interval: [-2.36833832  2.49661429]\n"
     ]
    }
   ],
   "source": [
    "item_sales_inferential(44)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The null hypothesis stands this time. Therefore, we cannot say the sales record for item 44 actually dropped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In extreme weather days, people buy more staff apart from item 44, item 45 and item 9, for last 6 items in top 10 increased by roughly 2 item per day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to above statements, the null hypotheses are tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item 5\n",
      "p value: 0.0047\n",
      "value 1.78574791356\n",
      "99% null hypothesis interval: [-1.72427105  1.75471021]\n",
      "\n",
      "\n",
      "item 68\n",
      "p value: 0.0\n",
      "value 4.97743431142\n",
      "99% null hypothesis interval: [-0.72214722  0.72498934]\n",
      "\n",
      "\n",
      "item 16\n",
      "p value: 0.0\n",
      "value 2.04496793016\n",
      "99% null hypothesis interval: [-0.73142313  0.77779933]\n",
      "\n",
      "\n",
      "item 25\n",
      "p value: 0.0\n",
      "value 2.35675080833\n",
      "99% null hypothesis interval: [-1.1457389   1.30350309]\n",
      "\n",
      "\n",
      "item 48\n",
      "p value: 0.0\n",
      "value 2.5980431391\n",
      "99% null hypothesis interval: [-0.86165149  0.97645239]\n",
      "\n",
      "\n",
      "item 36\n",
      "p value: 0.0\n",
      "value 1.51021767097\n",
      "99% null hypothesis interval: [-0.6277559   0.71786791]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print \"item 5\"\n",
    "item_sales_inferential(5)\n",
    "print \"\\n\"\n",
    "\n",
    "print \"item 68\"\n",
    "item_sales_inferential(68)\n",
    "print \"\\n\"\n",
    "\n",
    "print \"item 16\"\n",
    "item_sales_inferential(16)\n",
    "print \"\\n\"\n",
    "\n",
    "print \"item 25\"\n",
    "item_sales_inferential(25)\n",
    "print \"\\n\"\n",
    "\n",
    "print \"item 48\"\n",
    "item_sales_inferential(48)\n",
    "print \"\\n\"\n",
    "\n",
    "print \"item 36\"\n",
    "item_sales_inferential(36)\n",
    "print \"\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All null hypotheses do not stand, meaning that the statements are true: The sales records of these 6 items indeed increased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In extreme weather days, people buy item 5 more and they often do so before extreme weather comes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zexi/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:3: FutureWarning: pd.rolling_mean is deprecated for Series and will be removed in a future version, replace with \n",
      "\tSeries.rolling(window=3,center=False).mean()\n",
      "  app.launch_new_instance()\n",
      "/home/zexi/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:4: FutureWarning: pd.rolling_mean is deprecated for Series and will be removed in a future version, replace with \n",
      "\tSeries.rolling(window=3,center=False).mean()\n"
     ]
    }
   ],
   "source": [
    "mask = (df_ordered['WEvent'] == 1)\n",
    "\n",
    "df_ordered['Before_Event'] = (pd.rolling_mean(df_ordered['Condition'], window=3).shift(-3) > 0)\n",
    "df_ordered['After_Event'] = (pd.rolling_mean(df_ordered['Condition'], window=3).shift(1) > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p value: 0.0708\n",
      "value 2.06832645124\n",
      "99% null hypothesis interval: [-3.71796514  3.75806004]\n"
     ]
    }
   ],
   "source": [
    "mask_event = ((df_ordered['After_Event'] == 0) & (df_ordered['WEvent'] == 1) & (df_ordered['item_nbr'] == 5))\n",
    "mask_no_event = ((df_ordered['After_Event'] == 1) & (df_ordered['item_nbr'] == 5))\n",
    "\n",
    "data1 = list(df_ordered.loc[mask_event, 'units'])\n",
    "data2 = list(df_ordered.loc[mask_no_event, 'units'])\n",
    "\n",
    "perm_diff(data1,data2,np.mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This null hypothesis stands, although with a p-value of only 0.07."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### People buy item 93 more on the day of extreme weather. They also buy this item before an extreme weather.\n",
    "\n",
    "Two null hypotheses are tested here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p value: 0.0\n",
      "value 4.59785472258\n",
      "99% null hypothesis interval: [-0.33650113  0.55326266]\n"
     ]
    }
   ],
   "source": [
    "mask_event = ((df_ordered['Condition'] == 1) & (df_ordered['item_nbr'] == 93))\n",
    "mask_no_event = ((df_ordered['Condition'] == 0) & (df_ordered['item_nbr'] == 93))\n",
    "\n",
    "data1 = list(df_ordered.loc[mask_event, 'units'])\n",
    "data2 = list(df_ordered.loc[mask_no_event, 'units'])\n",
    "\n",
    "perm_diff(data1,data2,np.mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The null hypothesis does not stand. Item 93 did sell better on the day of extreme weather than the rest of days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p value: 0.0\n",
      "value 2.46105218936\n",
      "99% null hypothesis interval: [-0.22196152  0.30149219]\n"
     ]
    }
   ],
   "source": [
    "mask_event = ((df_ordered['Condition'] == 0) & (df_ordered['item_nbr'] == 93) & (df_ordered['Before_Event'] == 1))\n",
    "mask_no_event = ((df_ordered['Condition'] == 0) & (df_ordered['item_nbr'] == 93) & (df_ordered['Before_Event'] == 0))\n",
    "\n",
    "data1 = list(df_ordered.loc[mask_event, 'units'])\n",
    "data2 = list(df_ordered.loc[mask_no_event, 'units'])\n",
    "\n",
    "perm_diff(data1,data2,np.mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The null hypothesis does not stand. Item 93 did sell better shortly before the day of extreme weather than the rest of days excluding the extreme weather days."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When facing a long time extreme weather event, people do less shopping on item 5, 45, 44"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item 5: \n",
      "p value: 0.4257\n",
      "value -0.377943906968\n",
      "99% null hypothesis interval: [-5.33110912  5.13037168]\n",
      "item 45: \n",
      "p value: 0.1446\n",
      "value 1.94136738982\n",
      "99% null hypothesis interval: [-4.74552816  4.51252979]\n",
      "item 44: \n",
      "p value: 0.2166\n",
      "value 1.9207331672\n",
      "99% null hypothesis interval: [-6.63007756  5.91249994]\n"
     ]
    }
   ],
   "source": [
    "def examinie_long_term(item):\n",
    "\n",
    "    mask_event = ((df_ordered['Before_Event'] ^ df_ordered['After_Event'] == 1) & (df_ordered['item_nbr'] == item) & (df_ordered['WEvent'] == 1))\n",
    "    mask_no_event = ((df_ordered['Before_Event'] ^ df_ordered['After_Event'] == 0) & (df_ordered['item_nbr'] == item) & (df_ordered['WEvent'] == 1))\n",
    "\n",
    "    data1 = list(df_ordered.loc[mask_event, 'units'])\n",
    "    data2 = list(df_ordered.loc[mask_no_event, 'units'])\n",
    "\n",
    "    perm_diff(data1,data2,np.mean)\n",
    "    \n",
    "print \"item 5: \"\n",
    "examinie_long_term(5)\n",
    "print \"item 45: \"\n",
    "examinie_long_term(45)\n",
    "print \"item 44: \"\n",
    "examinie_long_term(44)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All null hypotheses do not stand, meaning the observation on this issue could be coincident."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Even when it is a sunny day, the sales record close to bad weather still differ from normal case, with item 5 being the best seller and item 45 at the third place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p value: 0.0285\n",
      "value 1.80844419214\n",
      "99% null hypothesis interval: [-2.25521036  2.46452891]\n"
     ]
    }
   ],
   "source": [
    "mask_event = ((df_ordered['preciptotal'] == 0) & (df_ordered['item_nbr'] == 5) & (df_ordered['WEvent'] == 1))\n",
    "mask_no_event = ((df_ordered['preciptotal'] == 0) & (df_ordered['item_nbr'] == 5) & (df_ordered['WEvent'] == 0))\n",
    "\n",
    "data1 = list(df_ordered.loc[mask_event, 'units'])\n",
    "data2 = list(df_ordered.loc[mask_no_event, 'units'])\n",
    "\n",
    "perm_diff(data1,data2,np.mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The null hypothesis rejected on 5% confidence level, meaning the statement stands: Even for sunny days, item 5 sells is dependent on if the day is close to an extreme weather day or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p value: 0.0\n",
      "value -7.18063549636\n",
      "99% null hypothesis interval: [-2.59934386  2.71724752]\n"
     ]
    }
   ],
   "source": [
    "mask_event = ((df_ordered['preciptotal'] == 0) & (df_ordered['item_nbr'] == 45) & (df_ordered['WEvent'] == 1))\n",
    "mask_no_event = ((df_ordered['preciptotal'] == 0) & (df_ordered['item_nbr'] == 45) & (df_ordered['WEvent'] == 0))\n",
    "\n",
    "data1 = list(df_ordered.loc[mask_event, 'units'])\n",
    "data2 = list(df_ordered.loc[mask_no_event, 'units'])\n",
    "\n",
    "perm_diff(data1,data2,np.mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The null hypothesis does not stand, meaning the statement stands: Even for sunny days, item 45 sells is dependent on if the day is close to an extreme weather day or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For item 5: Year: Sales record steady goes down given the year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2012 item 5 sales per day: \n",
      "value,  23.7681435507\n",
      "99% interval,  [ 23.00569809  24.53114453]\n",
      "None\n",
      "2013 item 5 sales per day: \n",
      "value,  18.8006975585\n",
      "99% interval,  [ 18.13004484  19.46658303]\n",
      "None\n",
      "2014 item 5 sales per day: \n",
      "value,  16.5693385352\n",
      "99% interval,  [ 15.92374401  17.24055728]\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df_ordered['year'] = pd.to_datetime(df_ordered['date'], infer_datetime_format=True).dt.year\n",
    "\n",
    "mask2012 = ((df_ordered['year'] == 2012) & (df_ordered['item_nbr'] == 5))\n",
    "mask2013 = ((df_ordered['year'] == 2013) & (df_ordered['item_nbr'] == 5))\n",
    "mask2014 = ((df_ordered['year'] == 2014) & (df_ordered['item_nbr'] == 5))\n",
    "\n",
    "data2012 = list(df_ordered.loc[mask2012,'units'])\n",
    "data2013 = list(df_ordered.loc[mask2013,'units'])\n",
    "data2014 = list(df_ordered.loc[mask2014,'units'])\n",
    "\n",
    "print \"2012 item 5 sales per day: \"\n",
    "print bsfromfunc(data2012,np.mean)\n",
    "\n",
    "print \"2013 item 5 sales per day: \"\n",
    "print bsfromfunc(data2013,np.mean)\n",
    "\n",
    "print \"2014 item 5 sales per day: \"\n",
    "print bsfromfunc(data2014,np.mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that 99% confidence intervals do not overlap between one another. It means the sales record is changing over the years. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For item 5: Weekday: People tend to buy more on weekends. On weekdays, Monday and Friday see more selling than others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average sales for Mon:\n",
      "value,  20.9906276151\n",
      "99% interval,  [ 19.91932636  22.09708954]\n",
      "None\n",
      "The average sales for Tue:\n",
      "value,  18.80074551\n",
      "99% interval,  [ 17.76243138  19.82431379]\n",
      "None\n",
      "The average sales for Wed:\n",
      "value,  16.9823766365\n",
      "99% interval,  [ 16.05638973  17.93992699]\n",
      "None\n",
      "The average sales for Thu:\n",
      "value,  16.6404380792\n",
      "99% interval,  [ 15.76763943  17.58113732]\n",
      "None\n",
      "The average sales for Fri:\n",
      "value,  18.3176391458\n",
      "99% interval,  [ 17.33327056  19.33951656]\n",
      "None\n",
      "The average sales for Sat:\n",
      "value,  22.8378972279\n",
      "99% interval,  [ 21.60175203  24.09754817]\n",
      "None\n",
      "The average sales for Sun:\n",
      "value,  26.3804256745\n",
      "99% interval,  [ 25.03401961  27.76337942]\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df_ordered['weekday'] = pd.to_datetime(df_ordered['date'], infer_datetime_format=True).dt.weekday\n",
    "\n",
    "maskMon = ((df_ordered['weekday'] == 0) & (df_ordered['item_nbr'] == 5))\n",
    "maskTue = ((df_ordered['weekday'] == 1) & (df_ordered['item_nbr'] == 5))\n",
    "maskWed = ((df_ordered['weekday'] == 2) & (df_ordered['item_nbr'] == 5))\n",
    "maskThu = ((df_ordered['weekday'] == 3) & (df_ordered['item_nbr'] == 5))\n",
    "maskFri = ((df_ordered['weekday'] == 4) & (df_ordered['item_nbr'] == 5))\n",
    "maskSat = ((df_ordered['weekday'] == 5) & (df_ordered['item_nbr'] == 5))\n",
    "maskSun = ((df_ordered['weekday'] == 6) & (df_ordered['item_nbr'] == 5))\n",
    "\n",
    "list_para = [maskMon,maskTue,maskWed,maskThu,maskFri,maskSat,maskSun]\n",
    "weekdays = ['Mon','Tue','Wed','Thu','Fri','Sat','Sun']\n",
    "\n",
    "for ind in range(len(list_para)):\n",
    "    data = list(df_ordered.loc[list_para[ind],'units'])\n",
    "    print 'The average sales for ' + weekdays[ind] + ':'\n",
    "    print bsfromfunc(data,np.mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The statement largely holds, for there is only a tiny bit confidence interval overlapping between weekday sales and weekend sales. However, we cannot say Friday sales better than other weekdays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For item 5: Rainfall/Snowfall: People tend to buy item 5 on a sunny day. But when facing major weather events people will go and buy them as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sunny vs non-sunny days in normal days\n",
      "p value: 0.0\n",
      "value 0.0539291156784\n",
      "99% null hypothesis interval: [-0.02658073  0.0260989 ]\n",
      "non-sunny normal days vs weather event days\n",
      "p value: 0.0\n",
      "value 0.0929169131328\n",
      "99% null hypothesis interval: [-0.05249095  0.05089615]\n"
     ]
    }
   ],
   "source": [
    "mask_normal_sunny = ((df_ordered['preciptotal'] == 0) & (df_ordered['WEvent'] == 0))\n",
    "mask_noraml_not_summy = ((df_ordered['preciptotal'] > 0) & (df_ordered['WEvent'] == 0))\n",
    "mask_weather = (df_ordered['WEvent'] == 1)\n",
    "\n",
    "data_noraml_sunny = list(df_ordered.loc[mask_normal_sunny,'units'])\n",
    "data_noraml_not_summy = list(df_ordered.loc[mask_noraml_not_summy,'units'])\n",
    "data_weather = list(df_ordered.loc[mask_weather,'units'])\n",
    "\n",
    "print \"sunny vs non-sunny days in normal days\"\n",
    "perm_diff(data_noraml_sunny,data_noraml_not_summy,np.mean)\n",
    "\n",
    "print \"non-sunny normal days vs weather event days\"\n",
    "perm_diff(data_weather,data_noraml_not_summy,np.mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two hypotheses do not stand. Meaning the statement is true: People cancel their shopping plans if there is light rain or snow, but they go shopping and stock up during weather events."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For item 5: Temperature: It can be observed that during normal days people tend to buy less item 5 when the temperature is between -16 to 38 $^{\\circ}$F, and between 64 $^{\\circ}$F to 76 $^{\\circ}$F. When there is a major weather event, however, the confidence interval becomes large enough to affect this conclusion from a statistical point of view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "I have already performed inferential statistics on this issue. Please see this part in data storytelling part2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "In conclusion: Apart from the point \"when facing a long time extreme weather event, people do less shopping on item 5, 45, 44\", the other point stands. Therefore the following conclusions in data storytelling stand: \n",
    "\n",
    "* The sales pattern of normal days is different from the sales pattern of extreme weather period.\n",
    "* Item 5 sells best during extreme weather period.\n",
    "* Features that indicating whether the day is a normal day, a day before a bad weather or a day after a bad weather could be useful since observation suggests that even when it is a sunny day, the sales record close to bad weather still differ from normal case.\n",
    "* Year: Year does affect the sales record.\n",
    "* Weekday: Weekday does affect the sales record.\n",
    "* Rainfall/Snowfall: Since we can indicate major events by event marker (see the part 1 report), whether rain/snow presents is useful in predicting item 5 sales, but the amount of rainfall/snowfall does not matter that much.\n",
    "* Temperature: During normal days, The temperature and selling record surely has a correlation in general case. However, this correlation is not linear. Also, the correlation between temperature and selling record during major weather events is less stronger than the one during normal days.\n",
    "\n",
    "\n",
    "While this conclusion does not stand:\n",
    "\n",
    "* There might be logical correlations between features that indicate whether the day is a normal day, a day before a bad weather or a day after a bad weather. Because of this, using neuro network on this project might be promising."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
