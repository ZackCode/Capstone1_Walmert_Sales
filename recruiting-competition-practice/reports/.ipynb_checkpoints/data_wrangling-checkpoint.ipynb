{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "58b63a5c-be37-4df0-a477-e9cf8218ba69",
    "_uuid": "76faab622382aaf0940bfb9739932334d1c2265b"
   },
   "source": [
    "### Step Two update - Aug 1st\n",
    "\n",
    "After talking to my supervisor, I want to familiar myself with formal procedures of data preprocessing. So I re-organize the document into current format. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "90f79b24-d55d-4dce-8c72-cdd901cd9068",
    "_execution_state": "idle",
    "_uuid": "5aa1986c22eed76f171c4dcf1458da7c1c80207e"
   },
   "source": [
    "### Step One update\n",
    "I am trying to improve my machine learning skills. For that, I create this notebook to take a finished competition here: https://www.kaggle.com/c/walmart-recruiting-sales-in-stormy-weather. The data is downloaded from the competition page and re-uploaded. For a detailed information regarding this dataset, one can refer to the competition page.\n",
    "\n",
    "This document archive my development progress on this project. More content will be added as my progress goes further. As a conclusion, all the codes will be transformed to Github. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3e45dc9f-dc7c-4f22-8d94-143da84c2283",
    "_execution_state": "idle",
    "_uuid": "bc794146f46f0b27dbcb4cc63b313e8f310b723a"
   },
   "source": [
    "# Preprocessing\n",
    "\n",
    "The procedure of preprocessing is as follows: data merge, data summarize, outliers detection and explaination, data interplation, data selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1f2f35cf-7d76-4d49-bfd8-72650f701806",
    "_uuid": "a1f279643542909de0a0955ed4f03ef49b7ffd81"
   },
   "source": [
    "# Data Merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d4809115-c73e-49d3-aee4-e61941463326",
    "_execution_state": "idle",
    "_uuid": "b3020aa349294847185b0877d14957f6f8469cf7"
   },
   "source": [
    "To start, the goal is to predict goods sales based on weather data. The sales record is stored in train.csv, and weather record is stored in weather.csv.  Data from key.csv indicates the corresponding relationship between the store and weather station.  So a logical step, to start the whole process, would be to merge the information from the datasets together.\n",
    "\n",
    "To merge datasets into one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "99a2ad34-d8fb-4a7d-9eda-1d6291407dce",
    "_execution_state": "idle",
    "_uuid": "2ed9eb95b2519eb4ae71ab1c9f9e56fc7586dc8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4617600, 4)\n",
      "(4617600, 5)\n",
      "(4617600, 23)\n",
      "['date', 'store_nbr', 'item_nbr', 'units', 'station_nbr', 'tmax', 'tmin', 'tavg', 'depart', 'dewpoint', 'wetbulb', 'heat', 'cool', 'sunrise', 'sunset', 'codesum', 'snowfall', 'preciptotal', 'stnpressure', 'sealevel', 'resultspeed', 'resultdir', 'avgspeed']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import re\n",
    "df_key = pd.read_csv(\"../input/key.csv\")\n",
    "df_train = pd.read_csv(\"../input/train.csv\")\n",
    "df_test = pd.read_csv(\"../input/test.csv\")\n",
    "df_weather = pd.read_csv(\"../input/weather.csv\")\n",
    "\n",
    "df_train['date'] = pd.to_datetime(df_train['date'])\n",
    "df_weather['date'] = pd.to_datetime(df_weather['date'])\n",
    "\n",
    "temp = pd.merge(df_train, df_key,how='left', on=['store_nbr'])\n",
    "df_main_train = pd.merge(temp, df_weather, how='left', on=['station_nbr','date'])\n",
    "\n",
    "print(df_train.shape)\n",
    "print(temp.shape)\n",
    "print(df_main_train.shape)\n",
    "print(list(df_main_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e03b58f8-cd79-4172-b083-7f4cf0d7f3e3",
    "_uuid": "e942588eff91373222502fceb0682528234f37d1"
   },
   "source": [
    "The weather station number is first merged to sales record based on store number, and both time and station number are used to merge sales record and weather record."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "96d1d3a1-f34f-44bd-85b2-7c61f8c00a18",
    "_execution_state": "idle",
    "_uuid": "434630bcb0a778bff79190255e6da83112ded475"
   },
   "source": [
    "## Data Summarize\n",
    "\n",
    "From above, we can see the following index appeared in the final dataset:\n",
    "\n",
    "date: year-month-day format\n",
    "store_nbr: Walmart store number\n",
    "item_nbr: item number, 117 of them, each number indicates one item, we do not have further information about what precise item would that be. \n",
    "units: number of item sold on that day\n",
    "station_nbr: weather station number\n",
    "tmax, tmin, tavg, depart, dewpoint, wetbulb: temperature max, min, average, departure from normal, average dew point, average wet bulb. in Fahrenheit\n",
    "\n",
    "heat, cool: not sure what it means\n",
    "\n",
    "sunrise, sunset: time of sunrise and sunset\n",
    "codesum: special code in letters indicating the weather conditions of that day, such as RA as rain, SN as snowing \n",
    "snowfall: snow/ice on ground in inches at 1200 UTC\n",
    "preciptotal: 24-hour snow drop in inches\n",
    "stnpressure: air pressure\n",
    "sealevel: in meters\n",
    "resultspeed: resultant wind speed, miles per hour\n",
    "resultdir: resultant wind direciton, in degrees\n",
    "avgspeed: average wind speed, miles per hour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "313dcd88-d887-47c4-ad57-2928f862bcac",
    "_execution_state": "idle",
    "_uuid": "9045311c9cb752fd7a0b837e5e016a1939059f29"
   },
   "source": [
    "The following is just getting familiar with dataset and pandas in general: ploting item 6 sales per month, snow drop per month, and daily sales vs snow drop plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d6b01cc0-18d1-470c-b572-2c8dac10e99c",
    "_execution_state": "idle",
    "_uuid": "1a67933f7e0e7653b6589a8fa15f50634955b23b"
   },
   "outputs": [],
   "source": [
    "df = df_main_train\n",
    "df['year'], df['month'] = df['date'].dt.year, df['date'].dt.month\n",
    "mask = (df['item_nbr'] == 6)\n",
    "df = df.loc[mask]\n",
    "\n",
    "df2 = df[['month','year','units']]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "count2 = df2.groupby(['month','year'])\n",
    "totalsum = count2['units'].aggregate(np.sum).unstack()\n",
    "##\n",
    "x = totalsum.values.reshape(-1,1)\n",
    "##\n",
    "totalsum.plot(kind = 'bar', title = 'units')\n",
    "plt.ylabel('count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "86225733-687a-4d74-87d8-55ce6eea9b7c",
    "_execution_state": "idle",
    "_uuid": "97ffc005fdab86863786abd4fbd1528370249d2e"
   },
   "outputs": [],
   "source": [
    "df3 = df[['preciptotal','month','year']]\n",
    "df3['preciptotal'] = df3['preciptotal'].convert_objects(convert_numeric=True)\n",
    "df3.interpolate()\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "count3 = df3.groupby(['month','year'])\n",
    "totalsum = count3['preciptotal'].aggregate(np.sum).unstack()\n",
    "##\n",
    "y = totalsum.values.reshape(-1,1)\n",
    "##\n",
    "totalsum.plot(kind = 'bar', title = 'preciptotal')\n",
    "plt.ylabel('count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "56f2999e-8c9e-4934-920f-e7640ddb1288",
    "_uuid": "3d496c4052ba1b7c91dcf3e0a64dd742e8b566a9"
   },
   "source": [
    "<font color='red'>convert_objects keeps giving me warning, any other alternatives? </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5b7e7e76-33d0-4b60-9cc1-b95c97c5cc3b",
    "_execution_state": "idle",
    "_uuid": "1f4b41a5f00925951d710227a91ca73398cf0226"
   },
   "outputs": [],
   "source": [
    "#plt.plot(x, y, 'o', label=\"data\")\n",
    "\n",
    "x1 = df2['units'].values.reshape(-1,1)\n",
    "y1 = df3['preciptotal'].values.reshape(-1,1)\n",
    "\n",
    "plt.plot(x1, y1, 'o', label=\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a915a3d4-6c9f-4b98-a3ae-2a64b1b2c525",
    "_execution_state": "idle",
    "_uuid": "472bda9ffa9a0391f57515e1724168195ac45dbc"
   },
   "source": [
    "## Weather Event Locate & Data interpolation\n",
    "\n",
    "Highlight the data for the weather events, which is defined as rainy days with 1 inch or more rainfall, or snowy days with 2 inch or more snowfall. \n",
    "\n",
    "<font color='red'>Problem: The goal here is not only focusing on event days, but 3 days before and after event days as well. What would be a convient way to mark those days</font> \n",
    "\n",
    "For data interpolation, pandas provide a convenient function: pd.interpolate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9b3ada8a-ef89-405c-b368-bbf52a2ae6d0",
    "_execution_state": "busy",
    "_uuid": "c95727501e3b64c7b5d44b1d90b8cbd7a4c2a5da"
   },
   "outputs": [],
   "source": [
    "df7 = df_main_train\n",
    "\n",
    "df7 = df7.convert_objects(convert_numeric=True)\n",
    "df7.interpolate()\n",
    "\n",
    "\n",
    "patternRA = 'RA'\n",
    "patternSN = 'SN'\n",
    "df7['RA'], df7['SN'] = df7['codesum'].str.contains(patternRA), df7['codesum'].str.contains(patternSN)\n",
    "df7['Condition'] = (df7['RA'] & (df7['preciptotal']>1.0)) | (df7['SN'] & (df7['preciptotal']>2.0))\n",
    "\n",
    "mask = (df7['Condition'] == True)\n",
    "df8 = df7.loc[mask]\n",
    "\n",
    "print(df8.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a7546318-eb2d-4922-9754-342762b2fd1a",
    "_execution_state": "idle",
    "_uuid": "9102bc708f895f2419803856eead4a97670d67a4"
   },
   "source": [
    "## Outlier Detection\n",
    "\n",
    "Looking for outliers, defined by numbers 3 std away from the main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6fa6b5fb-41ee-4611-8d51-299226cee9c3",
    "_execution_state": "busy",
    "_uuid": "a485e9c57fea11c0013b88b4b0f0b2126568ca99"
   },
   "outputs": [],
   "source": [
    "df9 = df8[['date','preciptotal']]\n",
    "\n",
    "df9.preciptotal.mean()\n",
    "\n",
    "df10 = df9[np.abs(df9.preciptotal-df9.preciptotal.mean())>(3*df9.preciptotal.std())]\n",
    "\n",
    "grouped_df = df10.groupby(['preciptotal'])['date']\n",
    "\n",
    "for key, item in grouped_df:\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ca858eac-7e97-4996-8987-46323262b09a",
    "_uuid": "5cfc58572fb9ac3aa131e001c1904c2ccbf0571c"
   },
   "source": [
    "As the most important data, 7.36 inch rainfall seems to be ok?..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1696e7c2-14fc-41f4-b201-0595b3179d2c",
    "_execution_state": "busy",
    "_uuid": "b8ccaa9d097056757af86e089b1ccf8fa349f49e"
   },
   "outputs": [],
   "source": [
    "df9 = df8[['date','tavg']]\n",
    "\n",
    "df9['tavg'] = df9['tavg'].convert_objects(convert_numeric=True)\n",
    "df9.interpolate()\n",
    "\n",
    "df9.tavg.mean()\n",
    "\n",
    "df10 = df9[np.abs(df9.tavg-df9.tavg.mean())>(3*df9.tavg.std())]\n",
    "\n",
    "grouped_df = df10.groupby(['tavg'])['date']\n",
    "\n",
    "for key, item in grouped_df:\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "32934ed3-ce4d-46f2-aa31-c663290aac02",
    "_uuid": "089f1d906b35166aff5ac57c4701e94c3c1342f5"
   },
   "source": [
    "-4 degree is the coldest case, as one lived in north Canada I envy those guys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "41d1baec-cf3c-42aa-a014-0594c157d3b8",
    "_execution_state": "busy",
    "_uuid": "89c9de91cf136376b5d74fc0153befb57167f13e"
   },
   "outputs": [],
   "source": [
    "df9 = df8[['date','avgspeed']]\n",
    "\n",
    "df9['avgspeed'] = df9['avgspeed'].convert_objects(convert_numeric=True)\n",
    "df9.interpolate()\n",
    "\n",
    "df9.avgspeed.mean()\n",
    "\n",
    "df10 = df9[np.abs(df9.avgspeed-df9.avgspeed.mean())>(3*df9.avgspeed.std())]\n",
    "\n",
    "grouped_df = df10.groupby(['avgspeed'])['date']\n",
    "\n",
    "for key, item in grouped_df:\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "36ac0891-f157-40dd-a9b6-4e88c200b7bc",
    "_uuid": "4ff619a22c3566073deb081f09224b44c5a26bad"
   },
   "source": [
    "nothing found at windspeed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9d8ca9ec-9cb6-42b9-982a-4c24cc29f7e3",
    "_execution_state": "idle",
    "_uuid": "9408c2cdd33c418319601c1aace373ef05fb5940"
   },
   "source": [
    "## Data Selection: VIF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "283f8e83-c861-4782-bb0f-5accdd86bce5",
    "_uuid": "17cb2210e5601363da2c39871cc67f5f31af21c1"
   },
   "source": [
    "<font color='red'>Problem: Not sure how to logically use it, so I make three groups based on tempature, wind, and rainfall/snowfall</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1650b68-512c-461c-9b97-9aafbfc4f329",
    "_execution_state": "busy",
    "_uuid": "3aae1e4036f489b108351858d5fca1162f234356"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from patsy import dmatrices\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "df11 = df8\n",
    "\n",
    "mask = (df11['item_nbr'] == 11)\n",
    "df11 = df11.loc[mask]\n",
    "\n",
    "df12 = df11[['units','tmax','tmin','tavg','depart','dewpoint','wetbulb','heat','cool']]\n",
    "df12 = df12.convert_objects(convert_numeric=True).dropna()\n",
    "df12 = df12._get_numeric_data()\n",
    "df12.reset_index(drop=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "921eb7f5-cf8e-4fc4-9766-bc106d2c8b17",
    "_execution_state": "busy",
    "_uuid": "81500921d6ff9b93b177f495c2ecb6a6415f7f40"
   },
   "outputs": [],
   "source": [
    "df13 = df12[['tmax','tmin','tavg','depart','dewpoint','wetbulb','heat','cool']]\n",
    "\n",
    "features = \"+\".join(df13.columns)\n",
    "y, X = dmatrices('units ~' + features, df12, return_type='dataframe')\n",
    "\n",
    "vif = pd.DataFrame()\n",
    "vif[\"VIF Factor\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "vif[\"features\"] = X.columns\n",
    "\n",
    "vif.round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "820aa4d2-7583-4f7e-abfa-c321c997cff0",
    "_execution_state": "busy",
    "_uuid": "c22831ada52e12bb8d66825b4b27dbb0fdc21c00"
   },
   "outputs": [],
   "source": [
    "\n",
    "df11 = df8\n",
    "\n",
    "mask = (df11['item_nbr'] == 11)\n",
    "df11 = df11.loc[mask]\n",
    "\n",
    "df12 = df11[['units','snowfall','preciptotal']]\n",
    "df12 = df12.convert_objects(convert_numeric=True).dropna()\n",
    "df12 = df12._get_numeric_data()\n",
    "df12.reset_index(drop=True)\n",
    "\n",
    "df13 = df12[['snowfall','preciptotal']]\n",
    "\n",
    "features = \"+\".join(df13.columns)\n",
    "y, X = dmatrices('units ~' + features, df12, return_type='dataframe')\n",
    "\n",
    "vif = pd.DataFrame()\n",
    "vif[\"VIF Factor\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "vif[\"features\"] = X.columns\n",
    "\n",
    "vif.round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6263c60d-38ec-4b8b-bfbf-584ca6323ee3",
    "_execution_state": "busy",
    "_uuid": "5ea99fd8db3db079ade84ce2c03203da3c378309"
   },
   "outputs": [],
   "source": [
    "\n",
    "df11 = df8\n",
    "\n",
    "mask = (df11['item_nbr'] == 11)\n",
    "df11 = df11.loc[mask]\n",
    "\n",
    "df12 = df11[['units','stnpressure','sealevel','resultspeed','resultdir','avgspeed']]\n",
    "df12 = df12.convert_objects(convert_numeric=True).dropna()\n",
    "df12 = df12._get_numeric_data()\n",
    "df12.reset_index(drop=True)\n",
    "\n",
    "df13 = df12[['stnpressure','sealevel','resultspeed','resultdir','avgspeed']]\n",
    "\n",
    "features = \"+\".join(df13.columns)\n",
    "y, X = dmatrices('units ~' + features, df12, return_type='dataframe')\n",
    "\n",
    "vif = pd.DataFrame()\n",
    "vif[\"VIF Factor\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "vif[\"features\"] = X.columns\n",
    "\n",
    "vif.round(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3cd7faeb-ae7a-4fec-9b76-eb0952ccfd20",
    "_uuid": "4a3c55c1e19f9dfcaed37c7f0a9b209d925b5a76"
   },
   "source": [
    "<font color='red'>Problem: How do I remove data based on this result?</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
